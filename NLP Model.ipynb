{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42001533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"en-fr.csv\"\n",
    "\n",
    "# Read the CSV file and sample 20000 rows\n",
    "sample_df = pd.read_csv(csv_file_path).sample(n=20000, random_state=42)\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer_input = Tokenizer(num_words=10000)\n",
    "tokenizer_target = Tokenizer(num_words=10000, filters='')\n",
    "\n",
    "# Convert all data to strings\n",
    "input_texts = [str(text) for text in sample_df.iloc[:, 0].tolist()]\n",
    "target_texts = ['<start> ' + str(text) for text in sample_df.iloc[:, 1].tolist()]\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer_input.fit_on_texts(input_texts)\n",
    "tokenizer_target.fit_on_texts(target_texts)\n",
    "\n",
    "# Adjusted Sequence Length\n",
    "sequence_length = 15  # Adjust this based on your analysis of the data\n",
    "\n",
    "input_sequences = tokenizer_input.texts_to_sequences(input_texts)\n",
    "input_data = pad_sequences(input_sequences, maxlen=sequence_length)\n",
    "\n",
    "target_sequences = tokenizer_target.texts_to_sequences(target_texts)\n",
    "target_data = pad_sequences(target_sequences, maxlen=sequence_length)\n",
    "\n",
    "# No need for one-hot encoding, use target_data directly with sparse_categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dfc1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Enhancements\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Assuming 'tokenizer_input' is for the encoder\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "embedding_dim = 256  # You can set this to the desired value\n",
    "vocab_size_target = 10000  # This should match the last dimension of your target data\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.Input(shape=(None,))\n",
    "enc_emb = tf.keras.layers.Embedding(vocab_size_input, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm1 = Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.4))(enc_emb)\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(tf.keras.layers.LSTM(256, return_state=True, dropout=0.4))(encoder_lstm1)\n",
    "state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
    "state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.Input(shape=(None,))\n",
    "dec_emb_layer = tf.keras.layers.Embedding(vocab_size_target, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm1 = tf.keras.layers.LSTM(512, return_sequences=True, dropout=0.4)(dec_emb, initial_state=encoder_states)  # Note: LSTM size doubled\n",
    "decoder_outputs = tf.keras.layers.LSTM(512, return_sequences=True, dropout=0.4)(decoder_lstm1)  # Note: LSTM size doubled\n",
    "\n",
    "# Attention Layer\n",
    "attention = tf.keras.layers.Attention()\n",
    "attn_out = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Fully connected layer\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_size_target, activation='softmax')\n",
    "decoder_outputs = decoder_dense(attn_out)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95edc3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "313/313 [==============================] - 543s 2s/step - loss: 6.4433\n",
      "Epoch 2/500\n",
      "313/313 [==============================] - 477s 2s/step - loss: 5.2949\n",
      "Epoch 3/500\n",
      "313/313 [==============================] - 398s 1s/step - loss: 4.7984\n",
      "Epoch 4/500\n",
      "313/313 [==============================] - 412s 1s/step - loss: 4.6634\n",
      "Epoch 5/500\n",
      "313/313 [==============================] - 409s 1s/step - loss: 4.5896\n",
      "Epoch 6/500\n",
      "313/313 [==============================] - 405s 1s/step - loss: 4.3494\n",
      "Epoch 7/500\n",
      "313/313 [==============================] - 413s 1s/step - loss: 4.2553\n",
      "Epoch 8/500\n",
      "313/313 [==============================] - 412s 1s/step - loss: 4.1524\n",
      "Epoch 9/500\n",
      "313/313 [==============================] - 409s 1s/step - loss: 4.0924\n",
      "Epoch 10/500\n",
      "313/313 [==============================] - 406s 1s/step - loss: 4.0221\n",
      "Epoch 11/500\n",
      "313/313 [==============================] - 407s 1s/step - loss: 3.9416\n",
      "Epoch 12/500\n",
      "313/313 [==============================] - 406s 1s/step - loss: 4.3317\n",
      "Epoch 13/500\n",
      "313/313 [==============================] - 408s 1s/step - loss: 4.1222\n",
      "Epoch 14/500\n",
      "313/313 [==============================] - 407s 1s/step - loss: 4.0153\n",
      "Epoch 15/500\n",
      "313/313 [==============================] - 407s 1s/step - loss: 3.8875\n",
      "Epoch 16/500\n",
      "313/313 [==============================] - 474s 2s/step - loss: 3.8115\n",
      "Epoch 17/500\n",
      "313/313 [==============================] - 518s 2s/step - loss: 3.7692\n",
      "Epoch 18/500\n",
      "313/313 [==============================] - 519s 2s/step - loss: 3.7259\n",
      "Epoch 19/500\n",
      "313/313 [==============================] - 519s 2s/step - loss: 3.6964\n",
      "Epoch 20/500\n",
      "313/313 [==============================] - 519s 2s/step - loss: 3.6446\n",
      "Epoch 21/500\n",
      "313/313 [==============================] - 517s 2s/step - loss: 3.4983\n",
      "Epoch 22/500\n",
      "313/313 [==============================] - 518s 2s/step - loss: 3.4209\n",
      "Epoch 23/500\n",
      "313/313 [==============================] - 514s 2s/step - loss: 3.3428\n",
      "Epoch 24/500\n",
      "313/313 [==============================] - 511s 2s/step - loss: 3.3019\n",
      "Epoch 25/500\n",
      "313/313 [==============================] - 511s 2s/step - loss: 3.2924\n",
      "Epoch 26/500\n",
      "313/313 [==============================] - 516s 2s/step - loss: 3.2345\n",
      "Epoch 27/500\n",
      "313/313 [==============================] - 523s 2s/step - loss: 3.1947\n",
      "Epoch 28/500\n",
      "313/313 [==============================] - 523s 2s/step - loss: 3.1905\n",
      "Epoch 29/500\n",
      "313/313 [==============================] - 519s 2s/step - loss: 3.0810\n",
      "Epoch 30/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 3.0386\n",
      "Epoch 31/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 2.9614\n",
      "Epoch 32/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 2.9153\n",
      "Epoch 33/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 2.8709\n",
      "Epoch 34/500\n",
      "313/313 [==============================] - 522s 2s/step - loss: 2.8591\n",
      "Epoch 35/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 2.8007\n",
      "Epoch 36/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 2.7472\n",
      "Epoch 37/500\n",
      "313/313 [==============================] - 529s 2s/step - loss: 2.7369\n",
      "Epoch 38/500\n",
      "313/313 [==============================] - 531s 2s/step - loss: 2.7019\n",
      "Epoch 39/500\n",
      "313/313 [==============================] - 530s 2s/step - loss: 2.6666\n",
      "Epoch 40/500\n",
      "313/313 [==============================] - 527s 2s/step - loss: 2.6163\n",
      "Epoch 41/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 2.5732\n",
      "Epoch 42/500\n",
      "313/313 [==============================] - 523s 2s/step - loss: 2.5573\n",
      "Epoch 43/500\n",
      "313/313 [==============================] - 528s 2s/step - loss: 2.5136\n",
      "Epoch 44/500\n",
      "313/313 [==============================] - 518s 2s/step - loss: 2.4904\n",
      "Epoch 45/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 2.4317\n",
      "Epoch 46/500\n",
      "313/313 [==============================] - 524s 2s/step - loss: 2.3878\n",
      "Epoch 47/500\n",
      "313/313 [==============================] - 524s 2s/step - loss: 2.3442\n",
      "Epoch 48/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 2.2906\n",
      "Epoch 49/500\n",
      "313/313 [==============================] - 523s 2s/step - loss: 2.2151\n",
      "Epoch 50/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 2.2358\n",
      "Epoch 51/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 2.1732\n",
      "Epoch 52/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 2.1414\n",
      "Epoch 53/500\n",
      "313/313 [==============================] - 530s 2s/step - loss: 2.1109\n",
      "Epoch 54/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 2.0689\n",
      "Epoch 55/500\n",
      "313/313 [==============================] - 524s 2s/step - loss: 2.0421\n",
      "Epoch 56/500\n",
      "313/313 [==============================] - 524s 2s/step - loss: 2.0115\n",
      "Epoch 57/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 1.9710\n",
      "Epoch 58/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 1.9456\n",
      "Epoch 59/500\n",
      "313/313 [==============================] - 520s 2s/step - loss: 1.9049\n",
      "Epoch 60/500\n",
      "313/313 [==============================] - 529s 2s/step - loss: 1.8724\n",
      "Epoch 61/500\n",
      "313/313 [==============================] - 522s 2s/step - loss: 1.8838\n",
      "Epoch 62/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 1.8157\n",
      "Epoch 63/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 1.7659\n",
      "Epoch 64/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 1.7605\n",
      "Epoch 65/500\n",
      "313/313 [==============================] - 501s 2s/step - loss: 1.7477\n",
      "Epoch 66/500\n",
      "313/313 [==============================] - 520s 2s/step - loss: 1.7164\n",
      "Epoch 67/500\n",
      "313/313 [==============================] - 523s 2s/step - loss: 1.7255\n",
      "Epoch 68/500\n",
      "313/313 [==============================] - 524s 2s/step - loss: 1.6893\n",
      "Epoch 69/500\n",
      "313/313 [==============================] - 527s 2s/step - loss: 1.6312\n",
      "Epoch 70/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 1.5977\n",
      "Epoch 71/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 1.6046\n",
      "Epoch 72/500\n",
      "313/313 [==============================] - 527s 2s/step - loss: 1.5747\n",
      "Epoch 73/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 1.5338\n",
      "Epoch 74/500\n",
      "313/313 [==============================] - 526s 2s/step - loss: 1.5449\n",
      "Epoch 75/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 1.5143\n",
      "Epoch 76/500\n",
      "313/313 [==============================] - 518s 2s/step - loss: 1.4971\n",
      "Epoch 77/500\n",
      "313/313 [==============================] - 505s 2s/step - loss: 1.4800\n",
      "Epoch 78/500\n",
      "313/313 [==============================] - 507s 2s/step - loss: 1.4251\n",
      "Epoch 79/500\n",
      "313/313 [==============================] - 527s 2s/step - loss: 1.4381\n",
      "Epoch 80/500\n",
      "313/313 [==============================] - 524s 2s/step - loss: 1.4021\n",
      "Epoch 81/500\n",
      "313/313 [==============================] - 527s 2s/step - loss: 1.4191\n",
      "Epoch 82/500\n",
      "313/313 [==============================] - 518s 2s/step - loss: 1.3625\n",
      "Epoch 83/500\n",
      "313/313 [==============================] - 524s 2s/step - loss: 1.3307\n",
      "Epoch 84/500\n",
      "313/313 [==============================] - 522s 2s/step - loss: 1.3311\n",
      "Epoch 85/500\n",
      "313/313 [==============================] - 521s 2s/step - loss: 1.3065\n",
      "Epoch 86/500\n",
      "313/313 [==============================] - 524s 2s/step - loss: 1.2994\n",
      "Epoch 87/500\n",
      "313/313 [==============================] - 498s 2s/step - loss: 1.2961\n",
      "Epoch 88/500\n",
      "313/313 [==============================] - 481s 2s/step - loss: 1.2755\n",
      "Epoch 89/500\n",
      "313/313 [==============================] - 512s 2s/step - loss: 1.2579\n",
      "Epoch 90/500\n",
      "313/313 [==============================] - 522s 2s/step - loss: 1.2604\n",
      "Epoch 91/500\n",
      "313/313 [==============================] - 525s 2s/step - loss: 1.2306\n",
      "Epoch 92/500\n",
      "313/313 [==============================] - 513s 2s/step - loss: 1.2062\n",
      "Epoch 93/500\n",
      "313/313 [==============================] - 443s 1s/step - loss: 1.1857\n",
      "Epoch 94/500\n",
      "313/313 [==============================] - 469s 1s/step - loss: 1.2067\n",
      "Epoch 95/500\n",
      "313/313 [==============================] - 466s 1s/step - loss: 1.1743\n",
      "Epoch 96/500\n",
      "313/313 [==============================] - 472s 2s/step - loss: 1.1456\n",
      "Epoch 97/500\n",
      "313/313 [==============================] - 504s 2s/step - loss: 1.1364\n",
      "Epoch 98/500\n",
      "313/313 [==============================] - 492s 2s/step - loss: 1.1117\n",
      "Epoch 99/500\n",
      "313/313 [==============================] - 478s 2s/step - loss: 1.1588\n",
      "Epoch 100/500\n",
      "313/313 [==============================] - 507s 2s/step - loss: 1.1061\n",
      "Epoch 101/500\n",
      "313/313 [==============================] - 498s 2s/step - loss: 1.0979\n",
      "Epoch 102/500\n",
      "313/313 [==============================] - 494s 2s/step - loss: 1.0820\n",
      "Epoch 103/500\n",
      "313/313 [==============================] - 466s 1s/step - loss: 1.0745\n",
      "Epoch 104/500\n",
      "313/313 [==============================] - 452s 1s/step - loss: 1.0461\n",
      "Epoch 105/500\n",
      "313/313 [==============================] - 450s 1s/step - loss: 1.0230\n",
      "Epoch 106/500\n",
      "313/313 [==============================] - 454s 1s/step - loss: 1.0193\n",
      "Epoch 107/500\n",
      "313/313 [==============================] - 435s 1s/step - loss: 1.0308\n",
      "Epoch 108/500\n",
      "313/313 [==============================] - 458s 1s/step - loss: 1.0177\n",
      "Epoch 109/500\n",
      "313/313 [==============================] - 463s 1s/step - loss: 0.9971\n",
      "Epoch 110/500\n",
      "313/313 [==============================] - 457s 1s/step - loss: 0.9810\n",
      "Epoch 111/500\n",
      "313/313 [==============================] - 450s 1s/step - loss: 0.9795\n",
      "Epoch 112/500\n",
      "313/313 [==============================] - 452s 1s/step - loss: 0.9682\n",
      "Epoch 113/500\n",
      "313/313 [==============================] - 446s 1s/step - loss: 0.9559\n",
      "Epoch 114/500\n",
      "313/313 [==============================] - 394s 1s/step - loss: 0.9667\n",
      "Epoch 115/500\n",
      "313/313 [==============================] - 394s 1s/step - loss: 0.9340\n",
      "Epoch 116/500\n",
      "313/313 [==============================] - 410s 1s/step - loss: 0.9295\n",
      "Epoch 117/500\n",
      "313/313 [==============================] - 404s 1s/step - loss: 0.9038\n",
      "Epoch 118/500\n",
      "313/313 [==============================] - 408s 1s/step - loss: 0.9510\n",
      "Epoch 119/500\n",
      "313/313 [==============================] - 406s 1s/step - loss: 1.2711\n",
      "Epoch 120/500\n",
      "313/313 [==============================] - 403s 1s/step - loss: 1.5443\n",
      "Epoch 121/500\n",
      "313/313 [==============================] - 410s 1s/step - loss: 1.1832\n",
      "Epoch 122/500\n",
      "313/313 [==============================] - 408s 1s/step - loss: 1.1346\n",
      "Epoch 123/500\n",
      "313/313 [==============================] - 444s 1s/step - loss: 1.1259\n",
      "Epoch 124/500\n",
      "313/313 [==============================] - 501s 2s/step - loss: 1.0142\n",
      "Epoch 125/500\n",
      "313/313 [==============================] - 449s 1s/step - loss: 0.9308\n",
      "Epoch 126/500\n",
      "313/313 [==============================] - 459s 1s/step - loss: 0.9236\n",
      "Epoch 127/500\n",
      "313/313 [==============================] - 463s 1s/step - loss: 0.9111\n",
      "Epoch 127: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "\n",
    "# --- Model Training ---\n",
    "model.fit([input_data, target_data], target_data, batch_size=64, epochs=500, callbacks=[early_stopping])\n",
    "\n",
    "# --- Beam Search Decoder Function ---\n",
    "def beam_search_decoder(data, k, tokenizer):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -np.log(row[j] + 1e-6)]\n",
    "                all_candidates.append(candidate)\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:k]\n",
    "    word_sequences = []\n",
    "    for seq, score in sequences:\n",
    "        words = [tokenizer.index_word.get(i, '?') for i in seq]\n",
    "        word_sequences.append((' '.join(words), score))\n",
    "    return word_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "859df84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Generated Sequence: <start> alimentaires alimentaires alimentaires alimentaires alimentaires alimentaires alimentaires alimentaires alimentaires\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to apply temperature scaling to predictions\n",
    "def temperature_scaled_prediction(predictions, temperature=1.0):\n",
    "    predictions = np.log(predictions + 1e-6) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    return exp_preds / np.sum(exp_preds)\n",
    "\n",
    "# Modified beam search decoder with temperature control\n",
    "def beam_search_decoder(data, k, tokenizer, temperature=1.0):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    for row in data:\n",
    "        scaled_row = temperature_scaled_prediction(row, temperature)\n",
    "        all_candidates = list()\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(scaled_row)):\n",
    "                candidate = [seq + [j], score * -np.log(scaled_row[j] + 1e-6)]\n",
    "                all_candidates.append(candidate)\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:k]\n",
    "    word_sequences = []\n",
    "    for seq, score in sequences:\n",
    "        words = [tokenizer.index_word.get(i, '?') for i in seq]\n",
    "        word_sequences.append((' '.join(words), score))\n",
    "    return word_sequences\n",
    "\n",
    "\n",
    "# Assuming the maximum sequence length used during data preparation is 10\n",
    "max_seq_length = 10\n",
    "\n",
    "some_input_data = input_data[0:1]\n",
    "\n",
    "# Initialize the decoder input as a zero matrix with shape (1, max_seq_length)\n",
    "decoder_input = np.zeros((1, max_seq_length))\n",
    "decoder_input[0, 0] = tokenizer_target.word_index['<start>']\n",
    "\n",
    "# Generating sequence one word at a time\n",
    "for i in range(max_seq_length - 1):\n",
    "    prediction = model.predict([some_input_data, decoder_input])\n",
    "    # Using a beam search with temperature scaling\n",
    "    beam_results = beam_search_decoder(prediction[0, :i+1, :], k=30, tokenizer=tokenizer_target, temperature=2.5)\n",
    "    if i < len(beam_results) and beam_results[0][0]:\n",
    "        chosen_word = beam_results[0][0].split()[0]\n",
    "        chosen_word_index = tokenizer_target.word_index.get(chosen_word, 0)\n",
    "        decoder_input[0, i + 1] = chosen_word_index\n",
    "\n",
    "# Constructing the final sequence from the decoder input\n",
    "final_sequence = ' '.join(tokenizer_target.index_word.get(idx, '?') for idx in decoder_input[0] if idx > 0)\n",
    "print(\"Generated Sequence:\", final_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc107891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c1873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ed810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
